{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS/CMPSC 410 Sparing 2021\n",
    "## Instructor: Professor John Yen\n",
    "## TA: Rupesh Prajapati and Dongkuan Xu\n",
    "## Lab 6: Movie Recommendations Using Alternative Least Square\n",
    "## The goals of this lab are for you to be able to\n",
    "### - Use Alternating Least Squares (ALS) for recommending movies based on reviews of users\n",
    "### - Be able to understand the raionale for splitting data into training, validation, and testing.\n",
    "### - Be able to tune hyper-parameters of the ALS model in a systematic way.\n",
    "### - Be able to store the results of evaluating hyper-parameters\n",
    "### - Be able to select best hyper-parameters and evaluate the chosen model with testing data\n",
    "### - Be able to improve the efficiency through persist or cache\n",
    "### - Be able to develop and debug in ICDS Jupyter Lab\n",
    "### - Be able to run Spark-submit (Cluster Mode) in Bridges2 for large movie reviews dataset\n",
    "\n",
    "## Exercises: \n",
    "- Exercise 1: 5 points\n",
    "- Exercise 2: 5 points\n",
    "- Exercise 3: 5 points\n",
    "- Exercise 4: 10 points\n",
    "- Exercise 5: 5 points\n",
    "- Exercise 6: 15 points\n",
    "- Exercise 7: 30 points\n",
    "## Total Points: 75 points\n",
    "\n",
    "# Due: midnight, February 28, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission of Lab 6\n",
    "- 1. Completed Jupyter Notebook of Lab 6 (Lab6A.ipynb) for small movie review datasets (movies_2.csv, ratings_2.csv).\n",
    "- 2. Lab6B.py (for spark-submit on Bridges2, incorporated all improvements from Exercise 6, processes large movie reviews)\n",
    "- 3. The output file that has the best hyperparameter setting for the large movie ratings files.  \n",
    "- 4. The log file of spark-submit on Lab6B.py \n",
    "- 5. A Word File that discusses (1) your answer to Exercise 6, and (2) your results of Exercise 7, including screen shots of your run-time information in the log file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first thing we need to do in each Jupyter Notebook running pyspark is to import pyspark first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once we import pyspark, we need to import \"SparkContext\".  Every spark program needs a SparkContext object\n",
    "### In order to use Spark SQL on DataFrames, we also need to import SparkSession from PySpark.SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import col, column\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, IndexToString\n",
    "# from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We then create a Spark Session variable (rather than Spark Context) in order to use DataFrame. \n",
    "- Note: We temporarily use \"local\" as the parameter for master in this notebook so that we can test it in ICDS Roar.  However, we need to change \"local\" to \"Yarn\" before we submit it to XSEDE to run in cluster mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=SparkSession.builder.appName(\"lab6\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (5 points) (a) Add your name below AND (b) replace the path below with the path of your home directory.\n",
    "## Answer for Exercise 1\n",
    "- a: Your Name: \n",
    "### Kangdong Yuan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (5 points) Modify the pathnames so that you can read the input CSV files (movies_2 and ratings_2 from ICDS Jupyter Lab) from the correct location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_DF = ss.read.csv(\"movies_2.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_DF = ss.read.csv(\"ratings_2.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings2_DF = ratings_DF.select(\"UserID\",\"MovieID\",\"Rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings2_DF.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings2_RDD = ratings2_DF.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Split Data into Three Sets: Training Data, Evaluatiion Data, and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_RDD, validation_RDD, test_RDD = ratings2_RDD.randomSplit([3, 1, 1], 137)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input (UserID, MovieID) for validation and for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_input_RDD = validation_RDD.map(lambda x: (x[0], x[1])) \n",
    "testing_input_RDD = test_RDD.map(lambda x: (x[0], x[1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Iterate through all possible combination of a set of values for three hyperparameters for ALS Recommendation Model:\n",
    "- rank (k)\n",
    "- regularization\n",
    "- iterations \n",
    "## Each hyperparameter value combination is used to construct an ALS recommendation model using training data, but evaluate using Evaluation Data\n",
    "## The evaluation results are saved in a Pandas DataFrame \n",
    "``\n",
    "hyperparams_eval_df\n",
    "``\n",
    "## The best hyperprameter value combination is stored in 4 variables\n",
    "``\n",
    "best_k, best_regularization, best_iterations, and lowest_validation_error\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# improve the performance by use presist() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[9580] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_RDD.persist()\n",
    "validation_input_RDD.persist()\n",
    "validation_RDD.persist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 (15 points) Complete the code below to iterate through a set of hyperparameters to create and evaluate ALS recommendation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best rank k is  4 , regularization =  0.1 , iterations =  20 . Validation Error = 0.938573557871417\n"
     ]
    }
   ],
   "source": [
    "## Initialize a Pandas DataFrame to store evaluation results of all combination of hyper-parameter settings\n",
    "hyperparams_eval_df = pd.DataFrame( columns = ['k', 'regularization', 'iterations', 'validation RMS', 'testing RMS'] )\n",
    "# initialize index to the hyperparam_eval_df to 0\n",
    "index =0 \n",
    "# initialize lowest_error\n",
    "lowest_validation_error = float('inf')\n",
    "# Set up the possible hyperparameter values to be evaluated\n",
    "iterations_list = [10, 15, 20]\n",
    "regularization_list = [0.01, 0.05, 0.1]\n",
    "rank_list = [4, 8, 12]\n",
    "for k in rank_list:\n",
    "    for regularization in regularization_list:\n",
    "        for iterations in iterations_list:\n",
    "            seed = 37\n",
    "            # Construct a recommendation model using a set of hyper-parameter values and training data\n",
    "            model = ALS.train(training_RDD, k, seed=seed, iterations=iterations, lambda_=regularization)\n",
    "            # Evaluate the model using evalution data\n",
    "            # map the output into ( (userID, movieID), rating ) so that we can join with actual evaluation data\n",
    "            # using (userID, movieID) as keys.\n",
    "            validation_prediction_RDD= model.predictAll(validation_input_RDD).map(lambda x: ( (x[0], x[1]), x[2] )   )\n",
    "            validation_evaluation_RDD = validation_RDD.map(lambda y: ( (y[0], y[1]), y[2]) ).join(validation_prediction_RDD)\n",
    "            # Calculate RMS error between the actual rating and predicted rating for (userID, movieID) pairs in validation dataset\n",
    "            error = math.sqrt(validation_evaluation_RDD.map(lambda z: (z[1][0] - z[1][1])**2).mean())\n",
    "            # Save the error as a row in a pandas DataFrame\n",
    "            hyperparams_eval_df.loc[index] = [k, regularization, iterations, error, float('inf')]\n",
    "            index = index + 1\n",
    "            # Check whether the current error is the lowest\n",
    "            if error < lowest_validation_error:\n",
    "                best_k = k\n",
    "                best_regularization = regularization\n",
    "                best_iterations = iterations\n",
    "                best_index = index\n",
    "                lowest_validation_error = error\n",
    "\n",
    "print('The best rank k is ', best_k, ', regularization = ', best_regularization, ', iterations = ',\\\n",
    "      best_iterations, '. Validation Error =', lowest_validation_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(hyperparams_eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Testing Data to Evaluate the Model built using the Best Hyperparameters                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 Evaluate the best hyperparameter combination using testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 (10 points)\n",
    "Complete the code below to evaluate the best hyperparameter combinations using testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Testing Error for rank k = 4  regularization =  0.1 , iterations =  20  is :  0.9395326595251623\n"
     ]
    }
   ],
   "source": [
    "seed = 37\n",
    "model = ALS.train(training_RDD, 4, seed=seed, iterations=20, lambda_=0.1)\n",
    "testing_prediction_RDD=model.predictAll(testing_input_RDD).map(lambda x: ((x[0], x[1]), x[2]))\n",
    "testing_evaluation_RDD= test_RDD.map(lambda x: ((x[0], x[1]), x[2])).join(testing_prediction_RDD)\n",
    "testing_error = math.sqrt(testing_evaluation_RDD.map(lambda x: (x[1][0] - x[1][1])**2).mean())\n",
    "print('The Testing Error for rank k =', best_k, ' regularization = ', best_regularization, ', iterations = ', \\\n",
    "      best_iterations, ' is : ', testing_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(best_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the Testing RMS in the DataFrame\n",
    "hyperparams_eval_df.loc[best_index]=[best_k, best_regularization, best_iterations, lowest_validation_error, testing_error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema3= StructType([ StructField(\"k\", FloatType(), True), \\\n",
    "                      StructField(\"regularization\", FloatType(), True ), \\\n",
    "                      StructField(\"iterations\", FloatType(), True), \\\n",
    "                      StructField(\"Validation RMS\", FloatType(), True), \\\n",
    "                      StructField(\"Testing RMS\", FloatType(), True) \\\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the pandas DataFrame that stores validation errors of all hyperparameters and the testing error for the best model to Spark DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperParams_Tuning_DF = ss.createDataFrame(hyperparams_eval_df, schema3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 (5 points)\n",
    "Modify the output path so that your output results can be saved in a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# projectPath=os.environ.get('PROJECT')\n",
    "# output_path = \"%s/Lab6output\"%projectPath\n",
    "# HyperParams_Tuning_DF.rdd.saveAsTextFile(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/storage/home/kky5082/ds410/lab6/Lab6_Output\"\n",
    "HyperParams_Tuning_DF.rdd.saveAsTextFile(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 (15 points)\n",
    "Modify the code above to improve its performance for Big Data.  Describe briefly here what modificationas you made and the rationale of each of your modifications.  Include this in item 5 of Lab6 submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the train and tunning hyperparameters part, there are three for loops to test all the possible hyperparameters set. So, it reuse some rdd many times. When program reuse the rdd, it has to computate it again. But I add presist() function to store those rdd in memory or disk, the computer don't need to computate again when program reuse it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7 (30 points)\n",
    "Save a duplicate copy of this notebook, name it Lab6B. \n",
    "- (1) Make similar modifications to Lab6 to prepare the notebook for processing the large movie reviews in Bridges2:\n",
    "* Modify the paths for the two large input files \n",
    "* Modify the path for the output file\n",
    "- (2) Export the notebook as Executable scripts (.py) file\n",
    "- (3) Use scp to copy the file to Bridges2\n",
    "- (4) Run (and time) spark-submit of Lab6B.py on Bridges2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission of Lab 6\n",
    "- 1. Completed Jupyter Notebook of Lab 6 (Lab6A.ipynb) for small movie review datasets (movies_2.csv, ratings_2.csv).\n",
    "- 2. Lab6B.py (for spark-submit on Bridges2, incorporated all improvements from Exercise 6, processes large movie reviews)\n",
    "- 3. The output file that has the best hyperparameter setting for the large movie ratings files.  \n",
    "- 4. The log file of spark-submit on Lab6B.py \n",
    "- 5. A Word File that discusses (1) your answer to Exercise 6, and (2) your results of Exercise 7, including screen shots of your run-time information in the log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
