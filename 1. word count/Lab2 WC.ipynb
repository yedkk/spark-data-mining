{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS/CMPSC 410\n",
    "\n",
    "# Lab 2 Solution: A MapReduce Implementation of Basic WordCount\n",
    "\n",
    "# Spring 2021\n",
    "\n",
    "## Instructor: Professor John Yen\n",
    "## TA: Rupesh Prajapati and Dongkuan Xu\n",
    "## Student name: Kangdong Yuan\n",
    "\n",
    "## Learning Objectives:\n",
    "- Be able to install pyspark \n",
    "- Be able to use map and reduce in Spark to implement word count.\n",
    "- Be able to understand the difference between map/reduce in Spark and their similar counterparts in Python: Lazy Evaluation.\n",
    "\n",
    "## This lab include 3 exercises:\n",
    "\n",
    "- Exercise 1: 5 points\n",
    "- Exercise 2: 5 points\n",
    "- Exercise 3: 10 points\n",
    "- Exercise 4: 7 points\n",
    "- Exercise 5: 8 points\n",
    "- Exercise 6: 10 points\n",
    "\n",
    "## Total: 45 points\n",
    "## Due: midnight of Jan 31st (Sunday)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Given a Large Document, calculate the term frequency (TF) of all its words.\n",
    "## Real World Examples: Google's processing of new/updated webpages to calculate and index their TF for Search Engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: A Python Implementation of Word Count\n",
    "Before we introduce a PySpark implementation, let's first see a Python implementation that uses Python map function.  This can help us later to compare the Python implementation with a PySpark implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "isfOVEnL2FW1"
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import re\n",
    "def read_url(url):\n",
    "    return re.sub('\\\\s+', ' ', urlopen(url).read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "14ZVri3W2FW9"
   },
   "outputs": [],
   "source": [
    "# Read The Adventures of Huckleberry Finn\n",
    "huck_finn_url = 'http://introcs.cs.princeton.edu/python/33design/huckfinn.txt'\n",
    "huck_finn_text = read_url(huck_finn_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvkePpJkpzHP"
   },
   "source": [
    "# Split the text into words (also referred to as terms or tokens), separated by space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ifYkOSZqpnv9",
    "outputId": "25911c72-013c-4503-cfc3-1827dc1e1867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'The', 'Adventures', 'of', 'Huckleberry', 'Finn,', 'Complete', 'by', 'Mark', 'Twain', '(Samuel', 'Clemens)', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever.', 'You', 'may', 'copy', 'it,', 'give', 'it', 'away', 'or', 're-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project']\n"
     ]
    }
   ],
   "source": [
    "huck_finn_words = huck_finn_text.split(sep = ' ')\n",
    "print(huck_finn_words[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QS693B3sqaSE",
    "outputId": "4622a57c-e848-4203-888d-164f9bed6645"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 1], ['Project', 1], ['Gutenberg', 1], ['EBook', 1], ['of', 1], ['The', 1], ['Adventures', 1], ['of', 1], ['Huckleberry', 1], ['Finn,', 1], ['Complete', 1], ['by', 1], ['Mark', 1], ['Twain', 1], ['(Samuel', 1], ['Clemens)', 1], ['This', 1], ['eBook', 1], ['is', 1], ['for', 1], ['the', 1], ['use', 1], ['of', 1], ['anyone', 1], ['anywhere', 1], ['at', 1], ['no', 1], ['cost', 1], ['and', 1], ['with', 1], ['almost', 1], ['no', 1], ['restrictions', 1], ['whatsoever.', 1], ['You', 1], ['may', 1], ['copy', 1], ['it,', 1], ['give', 1], ['it', 1], ['away', 1], ['or', 1], ['re-use', 1], ['it', 1], ['under', 1], ['the', 1], ['terms', 1], ['of', 1], ['the', 1], ['Project', 1]]\n"
     ]
    }
   ],
   "source": [
    "word_1_pairs = list( map(lambda x: [x, 1], huck_finn_words) )\n",
    "print(word_1_pairs[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ttx7g16JsImQ"
   },
   "source": [
    "## We want to group all of these key value paris for the same word together so that we can calculate the total time each word occurs in the input text.\n",
    "\n",
    "## One way to achieve this is to transform the list of key value pairs into a Panda Dataframe, then use groupby function of Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ejm9B42TscsC",
    "outputId": "cceca602-0fe9-46e1-bef8-243c4b351170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Word  Count\n",
      "0             The      1\n",
      "1         Project      1\n",
      "2       Gutenberg      1\n",
      "3           EBook      1\n",
      "4              of      1\n",
      "...           ...    ...\n",
      "113340       hear      1\n",
      "113341      about      1\n",
      "113342        new      1\n",
      "113343    eBooks.      1\n",
      "113344                 1\n",
      "\n",
      "[113345 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame( word_1_pairs, columns=['Word', 'Count'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fg1VpKHL8B5c",
    "outputId": "4ef68bbb-b07d-45bb-99f2-56d3c1d4b7fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Count\n",
      "Word                 \n",
      "                    1\n",
      "!                  14\n",
      "!\"                  1\n",
      "!--I                1\n",
      "\"$200               1\n",
      "...               ...\n",
      "yourself--just      1\n",
      "yourself.\"          2\n",
      "yourselves          2\n",
      "yourselves?         1\n",
      "yuther              7\n",
      "\n",
      "[13833 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "word_total = df.groupby(['Word']).sum()\n",
    "print(word_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the word counts in descending order so that we can easily so what words have highest frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AnjlIv1a-fPJ"
   },
   "outputs": [],
   "source": [
    "sorted_WC = word_total.sort_values(by=['Count'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jfFmrSVB_nnI",
    "outputId": "88344b1d-93fb-46b4-fabd-aaccfe20205a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Count\n",
      "Word         \n",
      "and      6035\n",
      "the      4645\n",
      "I        3041\n",
      "a        2916\n",
      "to       2899\n",
      "was      1941\n",
      "of       1718\n",
      "it       1430\n",
      "he       1372\n",
      "in       1357\n",
      "you      1023\n",
      "that      892\n",
      "for       819\n",
      "on        768\n",
      "all       743\n",
      "but       738\n",
      "up        697\n",
      "we        674\n",
      "out       645\n",
      "so        626\n",
      "got       602\n",
      "they      577\n",
      "with      547\n",
      "his       533\n",
      "as        531\n",
      "me        496\n",
      "him       473\n",
      "no        461\n",
      "had       445\n",
      "she       413\n",
      "see       409\n",
      "down      409\n",
      "at        403\n",
      "said      399\n",
      "about     393\n",
      "my        383\n",
      "or        377\n",
      "would     368\n",
      "them      367\n",
      "be        363\n",
      "there     359\n",
      "then      337\n",
      "when      336\n",
      "if        336\n",
      "by        335\n",
      "get       331\n",
      "didn't    318\n",
      "says:     311\n",
      "what      310\n",
      "come      308\n"
     ]
    }
   ],
   "source": [
    "print(sorted_WC.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: A PySpark Implementaion\n",
    "\n",
    "## The first thing we need to do in each Jupyter Notebook running PySpark is to import PySpark first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once we import pyspark, we need to import an important object called \"SparkContext\".  Every spark program needs a SparkContext object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We then create a Spark Context variable.  Once we have a spark context variable, we can execute spark codes.\n",
    "## Note: We can not create another Spark Context after creating one in an application, unless you first terminate the Spark context using the command\n",
    "```\n",
    "sc.stop\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://comp-sc-0227.acib.production.int.aci.ics.psu.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Lab2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=Lab2>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc=SparkContext(\"local\", \"Lab2\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (5 points) (a) Add your name below AND (b) replace the path below with the path of your home directory.\n",
    "## Answer for Exercise 1\n",
    "- a: Your Name: Kangdong Yuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/storage/home/kky5082/ds410/lab2/words.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_RDD = sc.textFile(\"/storage/home/kky5082/ds410/lab2/words.txt\")\n",
    "text_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_RDD = text_RDD.flatMap(lambda line: line.strip().split(\" \"))\n",
    "word_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[3] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_pair_RDD = word_RDD.map(lambda word: (word, 1))\n",
    "word_pair_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_RDD = word_pair_RDD.reduceByKey(lambda a, b: a + b, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_word_RDD = word_count_RDD.map(lambda x : [x[1], x[0]] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_count_word_RDD = count_word_RDD.sortByKey(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 'State'], [7, 'and'], [7, 'the'], [7, 'of'], [4, 'Penn'], [3, 'University'], [3, 'a'], [3, 'campuses'], [3, 'in'], [3, 'campus,'], [3, 'College'], [3, 'has'], [3, 'located'], [2, 'The'], [2, 'is'], [2, 'university'], [2, 'as'], [2, 'Its'], [2, 'mission'], [2, 'education'], [2, 'Park'], [2, 'Law,'], [1, 'Pennsylvania'], [1, '(Penn'], [1, 'or'], [1, 'PSU)'], [1, 'state-related,'], [1, 'land-grant,'], [1, 'doctoral'], [1, 'with'], [1, 'facilities'], [1, 'throughout'], [1, 'Pennsylvania.'], [1, 'Founded'], [1, '1855'], [1, \"Farmers'\"], [1, 'High'], [1, 'School'], [1, 'Pennsylvania,'], [1, 'conducts'], [1, 'teaching,'], [1, 'research,'], [1, 'public'], [1, 'service.'], [1, 'instructional'], [1, 'includes'], [1, 'undergraduate,'], [1, 'graduate,'], [1, 'professional'], [1, 'continuing']]\n"
     ]
    }
   ],
   "source": [
    "ssorted_count_word_RDDorted_CW_list = sorted_count_word_RDD.collect()\n",
    "print(ssorted_count_word_RDDorted_CW_list[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/storage/home/kky5082/ds410/lab2/words.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[3] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_pair_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[9] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 1),\n",
       " ('Pennsylvania', 1),\n",
       " ('State', 1),\n",
       " ('University', 1),\n",
       " ('(Penn', 1),\n",
       " ('State', 1),\n",
       " ('or', 1),\n",
       " ('PSU)', 1),\n",
       " ('is', 1),\n",
       " ('a', 1),\n",
       " ('state-related,', 1),\n",
       " ('land-grant,', 1),\n",
       " ('doctoral', 1),\n",
       " ('university', 1),\n",
       " ('with', 1),\n",
       " ('campuses', 1),\n",
       " ('and', 1),\n",
       " ('facilities', 1),\n",
       " ('throughout', 1),\n",
       " ('Pennsylvania.', 1),\n",
       " ('Founded', 1),\n",
       " ('in', 1),\n",
       " ('1855', 1),\n",
       " ('as', 1),\n",
       " ('the', 1),\n",
       " (\"Farmers'\", 1),\n",
       " ('High', 1),\n",
       " ('School', 1),\n",
       " ('of', 1),\n",
       " ('Pennsylvania,', 1),\n",
       " ('Penn', 1),\n",
       " ('State', 1),\n",
       " ('conducts', 1),\n",
       " ('teaching,', 1),\n",
       " ('research,', 1),\n",
       " ('and', 1),\n",
       " ('public', 1),\n",
       " ('service.', 1),\n",
       " ('Its', 1),\n",
       " ('instructional', 1),\n",
       " ('mission', 1),\n",
       " ('includes', 1),\n",
       " ('undergraduate,', 1),\n",
       " ('graduate,', 1),\n",
       " ('professional', 1),\n",
       " ('and', 1),\n",
       " ('continuing', 1),\n",
       " ('education', 1),\n",
       " ('offered', 1),\n",
       " ('through', 1),\n",
       " ('resident', 1),\n",
       " ('instruction', 1),\n",
       " ('and', 1),\n",
       " ('online', 1),\n",
       " ('delivery.', 1),\n",
       " ('Its', 1),\n",
       " ('University', 1),\n",
       " ('Park', 1),\n",
       " ('campus,', 1),\n",
       " ('the', 1),\n",
       " ('flagship', 1),\n",
       " ('campus,', 1),\n",
       " ('lies', 1),\n",
       " ('within', 1),\n",
       " ('the', 1),\n",
       " ('Borough', 1),\n",
       " ('of', 1),\n",
       " ('State', 1),\n",
       " ('College', 1),\n",
       " ('and', 1),\n",
       " ('College', 1),\n",
       " ('Township.', 1),\n",
       " ('It', 1),\n",
       " ('has', 1),\n",
       " ('two', 1),\n",
       " ('law', 1),\n",
       " ('schools:', 1),\n",
       " ('Penn', 1),\n",
       " ('State', 1),\n",
       " ('Law,', 1),\n",
       " ('on', 1),\n",
       " ('the', 1),\n",
       " (\"school's\", 1),\n",
       " ('University', 1),\n",
       " ('Park', 1),\n",
       " ('campus,', 1),\n",
       " ('and', 1),\n",
       " ('Dickinson', 1),\n",
       " ('Law,', 1),\n",
       " ('located', 1),\n",
       " ('in', 1),\n",
       " ('Carlisle,', 1),\n",
       " ('90', 1),\n",
       " ('miles', 1),\n",
       " ('south', 1),\n",
       " ('of', 1),\n",
       " ('State', 1),\n",
       " ('College.', 1),\n",
       " ('The', 1),\n",
       " ('College', 1),\n",
       " ('of', 1),\n",
       " ('Medicine', 1),\n",
       " ('is', 1),\n",
       " ('located', 1),\n",
       " ('in', 1),\n",
       " ('Hershey.', 1),\n",
       " ('Penn', 1),\n",
       " ('State', 1),\n",
       " ('has', 1),\n",
       " ('another', 1),\n",
       " ('19', 1),\n",
       " ('commonwealth', 1),\n",
       " ('campuses', 1),\n",
       " ('and', 1),\n",
       " ('5', 1),\n",
       " ('special', 1),\n",
       " ('mission', 1),\n",
       " ('campuses', 1),\n",
       " ('located', 1),\n",
       " ('across', 1),\n",
       " ('the', 1),\n",
       " ('state.', 1),\n",
       " ('Penn', 1),\n",
       " ('State', 1),\n",
       " ('has', 1),\n",
       " ('been', 1),\n",
       " ('labeled', 1),\n",
       " ('one', 1),\n",
       " ('of', 1),\n",
       " ('the', 1),\n",
       " ('\"Public', 1),\n",
       " ('Ivies,\"', 1),\n",
       " ('a', 1),\n",
       " ('publicly', 1),\n",
       " ('funded', 1),\n",
       " ('university', 1),\n",
       " ('considered', 1),\n",
       " ('as', 1),\n",
       " ('providing', 1),\n",
       " ('a', 1),\n",
       " ('quality', 1),\n",
       " ('of', 1),\n",
       " ('education', 1),\n",
       " ('comparable', 1),\n",
       " ('to', 1),\n",
       " ('those', 1),\n",
       " ('of', 1),\n",
       " ('the', 1),\n",
       " ('Ivy', 1),\n",
       " ('League.', 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_pair_RDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (5 points) Modify the path so that you can save the output into your directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"/storage/home/kky5082/ds410/lab2/Lab2_WC_sorted_out.txt\"\n",
    "\n",
    "sorted_CW_list = sorted_count_word_RDD.saveAsTextFile(output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (10 points) Use PySpark Map and reduceByKey to implement a \"Word/Term Frequency\" calculation for the text from \"The Adventures of Huckleberry Finn\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "14ZVri3W2FW9"
   },
   "outputs": [],
   "source": [
    "# Read The Adventures of Huckleberry Finn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/storage/home/kky5082/ds410/lab2/huckfinn.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2_RDD = sc.textFile(\"/storage/home/kky5082/ds410/lab2/huckfinn.txt\")\n",
    "text2_RDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2_RDD = text2_RDD.flatMap(lambda line: line.strip().split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pair2_RDD = word2_RDD.map(lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count2_RDD = word_pair2_RDD.reduceByKey(lambda a, b: a + b, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_word2_RDD = word_count2_RDD.map(lambda x : [x[1], x[0]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_count_word2_RDD = count_word2_RDD.sortByKey(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_CW_list2 = sorted_count_word2_RDD.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file2 = \"/storage/home/kky5082/ds410/lab2/Lab2_HuckFinn_WC_sorted_out.txt\"\n",
    "\n",
    "sorted_CW_list2 = sorted_count_word2_RDD.saveAsTextFile(output_file2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 (7 points) Describe the difference between the output of a pySpark command for reading a textfile with the output of a python command for reading a textfile. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer for Exercise 4: When we call the reading textfile in python it return a string object, which is the string of textfile we extracted. If we print the slice of this object, it will print the readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "The Project Gutenberg EBook of The Adventures of Huckleberry Finn, Complete by Mark Twain (Samuel Cl\n"
     ]
    }
   ],
   "source": [
    "huck_finn_url = 'http://introcs.cs.princeton.edu/python/33design/huckfinn.txt'\n",
    "huck_finn_text = read_url(huck_finn_url)\n",
    "print(type(huck_finn_text))\n",
    "print(huck_finn_text[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer for Exercise 4: When we reading the text by pyspark command, it will return a RDD object, which is a lazy object that need to be computed. If we print this RDD object, it will print the basic information of the RDD object, but it will not print the content of the textfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "/storage/home/kky5082/ds410/lab2/huckfinn.txt MapPartitionsRDD[11] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "text2_RDD = sc.textFile(\"/storage/home/kky5082/ds410/lab2/huckfinn.txt\")\n",
    "print(type(text2_RDD))\n",
    "print(text2_RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 (8 points)  Describe how map and reduce in pyspark differs from their counter parts in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer for Exercise 5: \n",
    "### 1. The map and reduce in pyspark will return a lazy object, which will not be compute untill we use collect function at end. But in python, the result will be computed right after the excution of the function and return the result.\n",
    "### 2. In pyspark, the map and reduce partition the data, and these data could be computed on cluster which include many machine. But the map and reduce in python only can run on one machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 (10 points) Can map/reduce in Pyspark be used to process a massive dataset (that does not fit in a computer)? If so, why?  If not, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Exercise 6:Yes, the pyspark can be used to process massive datase, because pyspark is distributed data processing tool. Pyspark use the lazy object as it's return. The pyspark will compute untill program ask it to compute, so this feature allows program run on cluster which include many machine. The data will be partitioned to suitable size and send to each machine to compute, after the computation in each machine, the data will be reduce and collect to the root machine. So pyspark can handle lagre scale data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
