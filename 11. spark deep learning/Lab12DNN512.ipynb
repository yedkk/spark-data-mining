{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS/CMPSC 410 Sparing 2021\n",
    "## Instructor: Professor John Yen\n",
    "## TA: Rupesh Prajapati and Dongkuan Xu\n",
    "## Lab 12 (Bonus): Deep Learning\n",
    "## The goals of this lab are for you to be able to\n",
    "### - Use tensorflow and keras to implement a deep learning application (MNIST)\n",
    "### - Be able to assess the result of DNN learning using validation data\n",
    "### - Be able to identify potential overfitting risk of a DNN model\n",
    "### - Be able to reduce potential overfitting risk by adjusting epoch and size of batch\n",
    "### - Be able to compare learning outcomes of different DNN architectures\n",
    "\n",
    "## Exercises: \n",
    "- Exercise 1: 5 points\n",
    "- Exercise 2: 10 points\n",
    "- Exercise 3: 10 points\n",
    "- Exercise 4: 20 points\n",
    "- Exercise 5: 25 points\n",
    "\n",
    "## Total Bonus Points (Lab): 70 points\n",
    "\n",
    "# Due: midnight, April 30, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install tensorflow and keras\n",
    "The first thing to do is to install tensorflow and keras in your ICDS Roar environment.\n",
    "- Open a terminal window in Jupyter Lab\n",
    "- Type the following in ther terminal window \n",
    "```pip install tensorflow```\n",
    "- After the installation of tensorflow completes, type the following in the terminal window \n",
    "```pip install keras```\n",
    "- Wait until the installation completes. Then run the \"import tensorflow as tf\" in Jupyter Notebook and continue based on the instructions on Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 (5 points)\n",
    "Enter your name here: Kangdong yuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images2 = train_images.reshape(60000, 784)\n",
    "test_images2 = test_images.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images3 = train_images2.astype('float32')\n",
    "test_images3 = test_images2.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images3 /= 255\n",
    "test_images3 /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 (10 points)\n",
    "Specify the number of nodes in each hidden layer (3 hidden layers total) for the DNN architecture.\n",
    "Recommended number of hidden layers: 512. The last layer is the output layer. Hence, it should have\n",
    "10 nodes (one for each signal digit character)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential( [ \\\n",
    "                             tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\\\n",
    "                             tf.keras.layers.Dense(512, activation='relu'), \\\n",
    "                             tf.keras.layers.Dense(512, activation='relu'),\n",
    "                             tf.keras.layers.Dense(10, activation='softmax'), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.3984 - accuracy: 0.8810 - val_loss: 0.0957 - val_accuracy: 0.9702\n",
      "Epoch 2/30\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0833 - accuracy: 0.9736 - val_loss: 0.0883 - val_accuracy: 0.9732\n",
      "Epoch 3/30\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.0513 - accuracy: 0.9837 - val_loss: 0.0931 - val_accuracy: 0.9719\n",
      "Epoch 4/30\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0386 - accuracy: 0.9870 - val_loss: 0.0990 - val_accuracy: 0.9717\n",
      "Epoch 5/30\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.0311 - accuracy: 0.9899 - val_loss: 0.0850 - val_accuracy: 0.9783\n",
      "Epoch 6/30\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.0263 - accuracy: 0.9910 - val_loss: 0.0718 - val_accuracy: 0.9810\n",
      "Epoch 7/30\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0197 - accuracy: 0.9934 - val_loss: 0.0627 - val_accuracy: 0.9821\n",
      "Epoch 8/30\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0185 - accuracy: 0.9941 - val_loss: 0.0829 - val_accuracy: 0.9790\n",
      "Epoch 9/30\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.0145 - accuracy: 0.9955 - val_loss: 0.0784 - val_accuracy: 0.9808\n",
      "Epoch 10/30\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.0119 - accuracy: 0.9962 - val_loss: 0.0836 - val_accuracy: 0.9807\n",
      "Epoch 11/30\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.0175 - accuracy: 0.9946 - val_loss: 0.0843 - val_accuracy: 0.9821\n",
      "Epoch 12/30\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0166 - accuracy: 0.9950 - val_loss: 0.0790 - val_accuracy: 0.9822\n",
      "Epoch 13/30\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0119 - accuracy: 0.9964 - val_loss: 0.1194 - val_accuracy: 0.9758\n",
      "Epoch 14/30\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.0119 - accuracy: 0.9964 - val_loss: 0.0932 - val_accuracy: 0.9810\n",
      "Epoch 15/30\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.0125 - accuracy: 0.9962 - val_loss: 0.0802 - val_accuracy: 0.9833\n",
      "Epoch 16/30\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0073 - accuracy: 0.9978 - val_loss: 0.1123 - val_accuracy: 0.9789\n",
      "Epoch 17/30\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0126 - accuracy: 0.9962 - val_loss: 0.0934 - val_accuracy: 0.9827\n",
      "Epoch 18/30\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0090 - accuracy: 0.9972 - val_loss: 0.0898 - val_accuracy: 0.9839\n",
      "Epoch 19/30\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.0067 - accuracy: 0.9981 - val_loss: 0.1052 - val_accuracy: 0.9810\n",
      "Epoch 20/30\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0105 - accuracy: 0.9971 - val_loss: 0.1227 - val_accuracy: 0.9768\n",
      "Epoch 21/30\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0116 - accuracy: 0.9967 - val_loss: 0.1039 - val_accuracy: 0.9816\n",
      "Epoch 22/30\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.0106 - accuracy: 0.9970 - val_loss: 0.1095 - val_accuracy: 0.9782\n",
      "Epoch 23/30\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.1043 - val_accuracy: 0.9826\n",
      "Epoch 24/30\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.1095 - val_accuracy: 0.9815\n",
      "Epoch 25/30\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0125 - accuracy: 0.9968 - val_loss: 0.1193 - val_accuracy: 0.9810\n",
      "Epoch 26/30\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.0084 - accuracy: 0.9979 - val_loss: 0.0919 - val_accuracy: 0.9836\n",
      "Epoch 27/30\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0044 - accuracy: 0.9987 - val_loss: 0.1035 - val_accuracy: 0.9841\n",
      "Epoch 28/30\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0076 - accuracy: 0.9981 - val_loss: 0.1328 - val_accuracy: 0.9788\n",
      "Epoch 29/30\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0067 - accuracy: 0.9982 - val_loss: 0.1134 - val_accuracy: 0.9828\n",
      "Epoch 30/30\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0060 - accuracy: 0.9981 - val_loss: 0.1108 - val_accuracy: 0.9819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b59980bb0a0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images3, train_labels, batch_size=128, epochs=30, verbose=1, \\\n",
    "          validation_data=(test_images3, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 (10 points)\n",
    "- (a) Does the final DNN model learned indicate a risk for overfitting? Explain your answer.\n",
    "- (b) Do you believe terminate earlier can reduce the risk?  If so, what termination condition you may want to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers to Exercise 3\n",
    "- (a)The final DNN model has the overfitting happen. Since the loss measure for validation data is much worse than the loss measure for training data. Moreover, The loss measure for validation data decreases as training proceeds. For epoch 30, loss for training data is 0.0060, and loss for validation data is 0.1108. \n",
    "\n",
    "- (b) Yes, I think terminate earlier help us reduce overfitting. From my observation, when loss < 3* val_loss, it gain highest validation accuracy, and lowest val_loss. After this threshold, the val_loss increase, and validation accuracy decrease. So, I will set the terminate condition is loss < 3* val_loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 (20 points)\n",
    "Change the batch size to 1000, complete the following code (keeping the number of nodes in each layer identical to Exercise 2. Compare results of the learned DNN to that of the previous one. Answer the following questions in the Markdown cell at the bottom of the Notebook.\n",
    "- (a) Which DNN will you choose?  Why? (10 points)\n",
    "- (b) Does the second DNN has risk for orverfitting?  (5 points)\n",
    "- (c) Will your termination condition find a model with less risk for overfitting? (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential( [ \\\n",
    "                             tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\\\n",
    "                             tf.keras.layers.Dense(512, activation='relu'), \\\n",
    "                             tf.keras.layers.Dense(512, activation='relu'),\n",
    "                             tf.keras.layers.Dense(10, activation='softmax'), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "60/60 [==============================] - 4s 59ms/step - loss: 0.8258 - accuracy: 0.7684 - val_loss: 0.1740 - val_accuracy: 0.9474\n",
      "Epoch 2/30\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.1493 - accuracy: 0.9559 - val_loss: 0.1015 - val_accuracy: 0.9673\n",
      "Epoch 3/30\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.0878 - accuracy: 0.9746 - val_loss: 0.0818 - val_accuracy: 0.9742\n",
      "Epoch 4/30\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.0617 - accuracy: 0.9815 - val_loss: 0.0697 - val_accuracy: 0.9775\n",
      "Epoch 5/30\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.0377 - accuracy: 0.9889 - val_loss: 0.0681 - val_accuracy: 0.9787\n",
      "Epoch 6/30\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.0274 - accuracy: 0.9916 - val_loss: 0.0640 - val_accuracy: 0.9800\n",
      "Epoch 7/30\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.0226 - accuracy: 0.9937 - val_loss: 0.0686 - val_accuracy: 0.9796\n",
      "Epoch 8/30\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.0133 - accuracy: 0.9965 - val_loss: 0.0689 - val_accuracy: 0.9807\n",
      "Epoch 9/30\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.0108 - accuracy: 0.9973 - val_loss: 0.0712 - val_accuracy: 0.9799\n",
      "Epoch 10/30\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.0075 - accuracy: 0.9983 - val_loss: 0.0703 - val_accuracy: 0.9803\n",
      "Epoch 11/30\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.0697 - val_accuracy: 0.9810\n",
      "Epoch 12/30\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.0084 - accuracy: 0.9976 - val_loss: 0.0780 - val_accuracy: 0.9806\n",
      "Epoch 13/30\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.0051 - accuracy: 0.9984 - val_loss: 0.0824 - val_accuracy: 0.9792\n",
      "Epoch 14/30\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.0043 - accuracy: 0.9986 - val_loss: 0.0795 - val_accuracy: 0.9812\n",
      "Epoch 15/30\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.0066 - accuracy: 0.9980 - val_loss: 0.0973 - val_accuracy: 0.9774\n",
      "Epoch 16/30\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.0090 - accuracy: 0.9969 - val_loss: 0.0914 - val_accuracy: 0.9768\n",
      "Epoch 17/30\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.0881 - val_accuracy: 0.9817\n",
      "Epoch 18/30\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0969 - val_accuracy: 0.9786\n",
      "Epoch 19/30\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.0099 - accuracy: 0.9970 - val_loss: 0.0984 - val_accuracy: 0.9792\n",
      "Epoch 20/30\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.0071 - accuracy: 0.9977 - val_loss: 0.0894 - val_accuracy: 0.9803\n",
      "Epoch 21/30\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0888 - val_accuracy: 0.9809\n",
      "Epoch 22/30\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.0018 - accuracy: 0.9993 - val_loss: 0.0887 - val_accuracy: 0.9817\n",
      "Epoch 23/30\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.0892 - val_accuracy: 0.9815\n",
      "Epoch 24/30\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0895 - val_accuracy: 0.9815\n",
      "Epoch 25/30\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.0914 - val_accuracy: 0.9825\n",
      "Epoch 26/30\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.1117 - val_accuracy: 0.9773\n",
      "Epoch 27/30\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.0129 - accuracy: 0.9961 - val_loss: 0.1017 - val_accuracy: 0.9778\n",
      "Epoch 28/30\n",
      "60/60 [==============================] - 3s 56ms/step - loss: 0.0103 - accuracy: 0.9963 - val_loss: 0.0905 - val_accuracy: 0.9803\n",
      "Epoch 29/30\n",
      "60/60 [==============================] - 3s 55ms/step - loss: 0.0080 - accuracy: 0.9974 - val_loss: 0.0816 - val_accuracy: 0.9814\n",
      "Epoch 30/30\n",
      "60/60 [==============================] - 3s 55ms/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 0.0862 - val_accuracy: 0.9820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b59b6356eb0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(train_images3, train_labels, batch_size=1000, epochs=30, verbose=1, \\\n",
    "          validation_data=(test_images3, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers to Exercise 4 (20 points)\n",
    "- (a) (10 points) I will select DNN 6. There are three reason. First, it has the lowest val_loss in all training process. Second, the val_accuracy at DNN 6 is one of the highest score in whole training process. Second, the difference between loss and val_loss at DNN 6 is not as large as the following DNN. \n",
    "- (b) (5 points) I don't think 2 DNN has overfitting, because the val_loss still decrase in following DNN, and the val_loss is less than training loss.\n",
    "- (c) (5 points) Yes, it have the risk of overfitting, since the best DNN is 6. But my terminate condition will terminate at DNN 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 (25 points)\n",
    "Copy code above for training a DNN of only two layers, using the same number of nodes in each layer as you chose for Exercise 2 and 4, but use the batch size that gave the better result.\n",
    "- (a) What is the performance result of the DNN learned? (10 points)\n",
    "- (a) Will you choose this DNN over the one with three layers?  Why? (10 points)\n",
    "- (b) Compare the overfitting risk of this DNN with that of the previous two? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) I use different batch size and I find that the larger batch size (3000, 5000,20000) will reach the lowest val_loss very early, which save some times. But if I use small batch size, it will cause the val_loss does't change. So I find 20000 is a suitable batch size.\n",
    "## (b) I will use this two layers DNN, since it cost less time then three layer model. And, simple layer reduce the model bias for this data, which will reduce the overfitting.\n",
    "## (c) The simpler model like two layer DNN has lower bias and higher variance compared to three layer DNN. So, two layer DNN has lower risk for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = tf.keras.Sequential( [ \\\n",
    "                             tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\\\n",
    "                             tf.keras.layers.Dense(512, activation='relu'), \\\n",
    "                             tf.keras.layers.Dense(10, activation='softmax'), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3/3 [==============================] - 2s 724ms/step - loss: 5.5582e-06 - accuracy: 1.0000 - val_loss: 0.0959 - val_accuracy: 0.9859\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 2s 711ms/step - loss: 5.5461e-06 - accuracy: 1.0000 - val_loss: 0.0959 - val_accuracy: 0.9859\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 2s 697ms/step - loss: 5.5366e-06 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 0.9859\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 2s 696ms/step - loss: 5.5269e-06 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 0.9859\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 2s 830ms/step - loss: 5.5173e-06 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 0.9859\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 2s 752ms/step - loss: 5.5081e-06 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 0.9859\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 2s 708ms/step - loss: 5.4993e-06 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 0.9858\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 2s 708ms/step - loss: 5.4903e-06 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 0.9858\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 2s 714ms/step - loss: 5.4813e-06 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 0.9858\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 2s 707ms/step - loss: 5.4724e-06 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9858\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 2s 767ms/step - loss: 5.4636e-06 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9858\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 2s 694ms/step - loss: 5.4551e-06 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9858\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 2s 694ms/step - loss: 5.4465e-06 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9858\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 2s 698ms/step - loss: 5.4380e-06 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9858\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 2s 849ms/step - loss: 5.4290e-06 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9858\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 3s 890ms/step - loss: 5.4209e-06 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9858\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 2s 732ms/step - loss: 5.4122e-06 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9858\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 2s 897ms/step - loss: 5.4040e-06 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9858\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 2s 712ms/step - loss: 5.3958e-06 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9858\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 2s 880ms/step - loss: 5.3876e-06 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9858\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 2s 710ms/step - loss: 5.3792e-06 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9858\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 2s 707ms/step - loss: 5.3709e-06 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9858\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 2s 741ms/step - loss: 5.3629e-06 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9858\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 2s 750ms/step - loss: 5.3546e-06 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9858\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 2s 778ms/step - loss: 5.3464e-06 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9858\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 2s 814ms/step - loss: 5.3383e-06 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9858\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 3s 904ms/step - loss: 5.3302e-06 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9858\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 2s 788ms/step - loss: 5.3222e-06 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9858\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 2s 683ms/step - loss: 5.3142e-06 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9858\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 2s 674ms/step - loss: 5.3063e-06 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b595f6a5520>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(train_images3, train_labels, batch_size=20000, epochs=30, verbose=1, \\\n",
    "          validation_data=(test_images3, test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
